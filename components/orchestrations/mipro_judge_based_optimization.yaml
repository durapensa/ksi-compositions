name: mipro_judge_based_optimization
component_type: orchestration
version: 1.0.0
description: 'MIPRO optimization for game theory prompts using LLM-as-Judge evaluation.

  Implements the co-evolutionary bootstrap system where judges and strategies improve
  together.

  '
author: ksi_system
timestamp: 2025-01-18 13:00:00+00:00
agents:
  optimization_coordinator:
    component: components/core/system_orchestrator
    vars:
      agent_id: mipro_coordinator
    prompt: 'You coordinate MIPRO optimization using judge-based evaluation.


      ## MANDATORY: Start with:

      {"event": "agent:status", "data": {"agent_id": "{{agent_id}}", "status": "coordinator_initialized",
      "optimization_target": "{{target_component}}"}}


      Your role:

      1. Manage optimization phases (bootstrap, proposal, search)

      2. Track performance using relative rankings from judges

      3. Build Elo-style ratings from pairwise comparisons

      4. Guide search based on judge feedback patterns


      ## MANDATORY: Track optimization state:

      {"event": "state:entity:create", "data": {"type": "optimization_state", "id":
      "{{agent_id}}_state", "properties": {"phase": "bootstrap", "iterations": 0,
      "best_variant": null}}}

      '
  prompt_generator:
    component: components/personas/creative_thinker
    vars:
      agent_id: prompt_generator
      prompt_suffix: '

        Generate strategic prompt variations for game theory agents.

        Focus on:

        - Strategic sophistication and multi-level thinking

        - Clear explanation of reasoning

        - Adaptive behavior patterns

        - Theory of mind capabilities

        '
  tournament_runner:
    component: components/core/system_orchestrator
    vars:
      agent_id: tournament_runner
    prompt: 'You run game tournaments and collect transcripts for judge evaluation.


      ## MANDATORY: Start with:

      {"event": "agent:status", "data": {"agent_id": "{{agent_id}}", "status": "tournament_initialized"}}


      For each matchup:

      1. Spawn two game agents with different prompt variants

      2. Run the game (simplified, 10 rounds)

      3. Collect reasoning transcripts

      4. Package for judge comparison

      '
  judge_coordinator:
    component: components/core/system_orchestrator
    vars:
      agent_id: judge_coordinator
    prompt: 'You coordinate multiple specialized judges for comprehensive evaluation.


      ## MANDATORY: Start with:

      {"event": "agent:status", "data": {"agent_id": "{{agent_id}}", "status": "judge_coordinator_initialized"}}


      Manage:

      1. Strategic Depth Judge - evaluates reasoning sophistication

      2. Adaptation Judge - scores learning and flexibility

      3. Explanation Judge - rates clarity of strategic thinking

      4. Meta Judge - aggregates and resolves disagreements

      '
variables:
  target_component: '{{target_component|default:''components/personas/game_players/strategic_reasoner''}}'
  num_iterations: '{{num_iterations|default:10}}'
  variants_per_iteration: '{{variants_per_iteration|default:4}}'
  games_per_variant: '{{games_per_variant|default:3}}'
  coordinator_model: claude-cli/sonnet
  generator_model: claude-cli/sonnet
  player_model: claude-cli/sonnet
  judge_model: claude-cli/sonnet
orchestration_logic:
  strategy: "## Phase 1: Initialize Optimization\nSTATE elo_ratings = {}  # Track\
    \ relative performance\nSTATE prompt_variants = []\nSTATE comparison_history =\
    \ []\nSTATE current_iteration = 0\nSTATE convergence_threshold = 0.02\n\nTRACK\
    \ {\n  phase: \"initialization\",\n  target: \"{{target_component}}\",\n  optimization_method:\
    \ \"mipro_with_judges\",\n  iterations_planned: {{num_iterations}}\n}\n\n# Load\
    \ base component\nEVENT composition:get_component {\n  name: \"{{target_component}}\"\
    \n} AS base_component\n\n# Initialize with base variant\nAPPEND prompt_variants\
    \ {\n  id: \"base\",\n  content: base_component.content,\n  elo_rating: 1500,\n\
    \  games_played: 0,\n  comparison_wins: 0\n}\n\n## Phase 2: Bootstrap with Initial\
    \ Variants\nTRACK {phase: \"bootstrap\", message: \"Generating initial prompt\
    \ variants\"}\n\n# Generate initial variants\nSEND {\n  to: prompt_generator,\n\
    \  message: {\n    action: \"generate_variants\",\n    base_prompt: base_component.content,\n\
    \    count: {{variants_per_iteration}},\n    focus_areas: [\"strategic_reasoning\"\
    , \"adaptation\", \"explanation_quality\"],\n    variation_strategies: [\"emphasize_different_aspects\"\
    , \"restructure_instructions\", \"add_examples\"]\n  }\n}\n\nAWAIT {\n  from:\
    \ prompt_generator,\n  timeout: 60\n} AS initial_variants\n\n# Add variants to\
    \ pool\nFOREACH variant IN initial_variants.variants:\n  APPEND prompt_variants\
    \ {\n    id: \"variant_{{LENGTH(prompt_variants)}}\",\n    content: variant.content,\n\
    \    elo_rating: 1500,\n    games_played: 0,\n    comparison_wins: 0,\n    generation:\
    \ 0,\n    strategy: variant.strategy\n  }\n\n## Phase 3: Optimization Loop\nLOOP\
    \ iteration FROM 1 TO {{num_iterations}}:\n  STATE current_iteration = iteration\n\
    \  \n  TRACK {\n    phase: \"optimization\",\n    iteration: iteration,\n    active_variants:\
    \ LENGTH(prompt_variants),\n    top_elo: MAX(prompt_variants, v.elo_rating)\n\
    \  }\n  \n  # Select variants for tournament (top performers + exploration)\n\
    \  STATE tournament_variants = SELECT_FOR_TOURNAMENT(prompt_variants, 4)\n  \n\
    \  # Run pairwise comparisons\n  STATE comparisons_this_round = []\n  \n  FOREACH\
    \ pair IN COMBINATIONS(tournament_variants, 2):\n    TRACK {\n      event: \"\
    running_comparison\",\n      variant_a: pair[0].id,\n      variant_b: pair[1].id\n\
    \    }\n    \n    # Run game between variants\n    SEND {\n      to: tournament_runner,\n\
    \      message: {\n        action: \"run_game\",\n        variant_a: pair[0],\n\
    \        variant_b: pair[1],\n        game_type: \"prisoners_dilemma\",\n    \
    \    rounds: 10\n      }\n    }\n    \n    AWAIT {\n      from: tournament_runner,\n\
    \      timeout: 120\n    } AS game_result\n    \n    # Judge comparison\n    SEND\
    \ {\n      to: judge_coordinator,\n      message: {\n        action: \"compare_strategies\"\
    ,\n        transcript_a: game_result.transcript_a,\n        transcript_b: game_result.transcript_b,\n\
    \        judges: [\"strategic_depth\", \"adaptation\", \"explanation\"],\n   \
    \     aggregation: \"majority_vote\"\n      }\n    }\n    \n    AWAIT {\n    \
    \  from: judge_coordinator,\n      timeout: 90\n    } AS judge_verdict\n    \n\
    \    # Update Elo ratings\n    STATE winner_idx = IF(judge_verdict.winner == \"\
    strategy_a\", 0, 1)\n    STATE loser_idx = 1 - winner_idx\n    \n    STATE new_ratings\
    \ = UPDATE_ELO(\n      pair[winner_idx].elo_rating,\n      pair[loser_idx].elo_rating,\n\
    \      k_factor=32\n    )\n    \n    UPDATE prompt_variants WHERE id == pair[winner_idx].id:\n\
    \      elo_rating = new_ratings.winner\n      games_played += 1\n      comparison_wins\
    \ += 1\n    \n    UPDATE prompt_variants WHERE id == pair[loser_idx].id:\n   \
    \   elo_rating = new_ratings.loser\n      games_played += 1\n    \n    APPEND\
    \ comparisons_this_round {\n      winner: pair[winner_idx].id,\n      loser: pair[loser_idx].id,\n\
    \      confidence: judge_verdict.confidence,\n      key_factors: judge_verdict.key_factors,\n\
    \      improvement_insights: judge_verdict.improvements\n    }\n  \n  # Analyze\
    \ round results\n  STATE round_insights = ANALYZE_COMPARISONS(comparisons_this_round)\n\
    \  \n  # Generate new variants based on insights\n  STATE top_variants = SELECT_TOP(prompt_variants,\
    \ 2, by=\"elo_rating\")\n  \n  SEND {\n    to: prompt_generator,\n    message:\
    \ {\n      action: \"generate_improved\",\n      base_variants: top_variants,\n\
    \      insights: round_insights,\n      losing_patterns: EXTRACT_LOSING_PATTERNS(comparisons_this_round),\n\
    \      winning_patterns: EXTRACT_WINNING_PATTERNS(comparisons_this_round),\n \
    \     generation: iteration\n    }\n  }\n  \n  AWAIT {\n    from: prompt_generator,\n\
    \    timeout: 60\n  } AS new_variants\n  \n  # Add new variants\n  FOREACH variant\
    \ IN new_variants.variants:\n    APPEND prompt_variants {\n      id: \"variant_{{LENGTH(prompt_variants)}}\"\
    ,\n      content: variant.content,\n      elo_rating: 1500,  # Start at baseline\n\
    \      games_played: 0,\n      comparison_wins: 0,\n      generation: iteration,\n\
    \      strategy: variant.strategy,\n      parent: variant.parent_id\n    }\n \
    \ \n  # Prune poor performers (keep pool manageable)\n  IF LENGTH(prompt_variants)\
    \ > 10:\n    STATE sorted_variants = SORT(prompt_variants, by=\"elo_rating\",\
    \ descending=true)\n    STATE prompt_variants = SLICE(sorted_variants, 0, 8) \
    \ # Keep top 8\n  \n  # Check convergence\n  STATE rating_variance = VARIANCE(prompt_variants,\
    \ v.elo_rating)\n  IF rating_variance < convergence_threshold:\n    TRACK {\n\
    \      event: \"convergence_detected\",\n      iteration: iteration,\n      reason:\
    \ \"rating_stability\",\n      top_rating: MAX(prompt_variants, v.elo_rating)\n\
    \    }\n    BREAK\n\n## Phase 4: Final Evaluation and Selection\nTRACK {phase:\
    \ \"finalization\", message: \"Selecting best variant\"}\n\n# Run final tournament\
    \ with top 3\nSTATE finalists = SELECT_TOP(prompt_variants, 3, by=\"elo_rating\"\
    )\n\n# Comprehensive evaluation\nSEND {\n  to: judge_coordinator,\n  message:\
    \ {\n    action: \"final_evaluation\",\n    variants: finalists,\n    evaluation_depth:\
    \ \"comprehensive\",\n    include_meta_judge: true\n  }\n}\n\nAWAIT {\n  from:\
    \ judge_coordinator,\n  timeout: 180\n} AS final_evaluation\n\nSTATE best_variant\
    \ = SELECT_BEST(finalists, final_evaluation)\n\n# Create optimized component\n\
    EVENT composition:create_component {\n  name: \"{{target_component}}_judge_optimized\"\
    ,\n  content: best_variant.content,\n  metadata: {\n    optimization_method: \"\
    mipro_judge_based\",\n    original_component: \"{{target_component}}\",\n    final_elo:\
    \ best_variant.elo_rating,\n    games_played: best_variant.games_played,\n   \
    \ win_rate: best_variant.comparison_wins / best_variant.games_played,\n    iterations:\
    \ current_iteration,\n    key_improvements: final_evaluation.improvement_summary\n\
    \  }\n}\n\n# Generate optimization report\nSTATE optimization_summary = {\n  method:\
    \ \"MIPRO with LLM-as-Judge\",\n  iterations: current_iteration,\n  variants_tested:\
    \ LENGTH(comparison_history),\n  final_elo_spread: MAX(finalists, f.elo_rating)\
    \ - MIN(finalists, f.elo_rating),\n  winning_patterns: EXTRACT_WINNING_PATTERNS(comparison_history),\n\
    \  convergence_achieved: rating_variance < convergence_threshold\n}\n\nTRACK {\n\
    \  phase: \"complete\",\n  optimization_summary: optimization_summary,\n  best_variant_id:\
    \ best_variant.id,\n  improvement_over_base: best_variant.elo_rating - 1500\n\
    }\n\n# Request termination\nEVENT orchestration:request_termination {\n  reason:\
    \ \"MIPRO judge-based optimization complete\",\n  results: optimization_summary\n\
    }\n"
helpers:
  SELECT_FOR_TOURNAMENT: '# Select top performers + some exploration

    # 75% top by Elo, 25% random for diversity

    '
  UPDATE_ELO: '# Standard Elo rating update

    # K-factor of 32 for rapid adaptation

    '
  ANALYZE_COMPARISONS: '# Extract patterns from judge feedback

    # Identify what separates winners from losers

    '
  EXTRACT_WINNING_PATTERNS: '# Common factors in winning strategies

    '
  EXTRACT_LOSING_PATTERNS: '# Common weaknesses to avoid

    '
metadata:
  pattern_type: optimization
  optimization_method: mipro_judge_based
  evaluation_approach: pairwise_comparison
  domain: game_theory
  capabilities_demonstrated:
  - llm_as_judge_evaluation
  - elo_rating_system
  - co_evolutionary_optimization
  - pattern_extraction
  - convergence_detection
  tags:
  - mipro
  - optimization
  - judge_based
  - game_theory
  - elo_rating
performance:
  expected_duration: 20-40 minutes
  resource_usage: 4-6 concurrent agents
  success_metrics:
    convergence_rate: < 10 iterations typical
    elo_spread: '> 100 points between best and base'
    pattern_discovery: Clear winning strategies identified
