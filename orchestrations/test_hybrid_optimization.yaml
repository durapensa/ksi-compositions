name: test_hybrid_optimization
type: orchestration
version: 1.0.0
description: 'Simple test of hybrid optimization combining DSPy structured search
  with judge evaluation.

  Demonstrates how programmatic optimization and LLM-as-Judge can work together.

  '
author: ksi_system
timestamp: 2025-01-18 14:30:00+00:00
agents:
  coordinator:
    component: components/core/dsl_interpreter_orchestrator
    vars:
      agent_id: hybrid_coordinator
    prompt: 'You coordinate a hybrid optimization approach.


      ## MANDATORY: Start with:

      {"event": "agent:status", "data": {"agent_id": "{{agent_id}}", "status": "hybrid_coordinator_initialized"}}


      Your strategy:

      1. Use DSPy events for structured variant generation

      2. Use judges for nuanced evaluation

      3. Combine insights from both approaches

      4. Track which aspects each technique handles best

      '
  variant_generator:
    component: components/core/system_single_agent
    vars:
      agent_id: variant_generator
    prompt: 'Generate prompt variants using structured approaches.


      ## MANDATORY: Start with:

      {"event": "agent:status", "data": {"agent_id": "{{agent_id}}", "status": "generator_initialized"}}


      Apply systematic variation strategies:

      1. Parameter substitution (temperature, style, focus)

      2. Structural reorganization

      3. Instruction emphasis shifting

      4. Example augmentation


      ## MANDATORY: Report each variant:

      {"event": "state:entity:create", "data": {"type": "prompt_variant", "id": "variant_{{index}}",
      "properties": {"content": "...", "strategy": "...", "parameters": {...}}}}

      '
  quality_judge:
    component: components/personas/judges/game_theory_pairwise_judge
    vars:
      agent_id: quality_judge
      comparison_focus: strategic_reasoning_quality
variables:
  target_component: '{{target_component|default:''components/personas/game_players/strategic_reasoner''}}'
  num_variants: '{{num_variants|default:4}}'
orchestration_logic:
  strategy: "## Phase 1: Setup\nSTATE variants = []\nSTATE evaluations = {}\nSTATE\
    \ dspy_metrics = {}\nSTATE judge_rankings = {}\n\nTRACK {\n  phase: \"initialization\"\
    ,\n  target: \"{{target_component}}\",\n  approach: \"hybrid_dspy_judge\"\n}\n\
    \n# Load target\nEVENT composition:get_component {\n  name: \"{{target_component}}\"\
    \n} AS base_component\n\n## Phase 2: Structured Generation (DSPy-style)\nTRACK\
    \ {phase: \"structured_generation\", message: \"Using DSPy principles for variant\
    \ creation\"}\n\n# First, use optimization service to validate setup\nEVENT optimization:validate_setup\
    \ {\n  framework: \"dspy\",\n  component_type: base_component.frontmatter.component_type\n\
    } AS validation\n\n# Generate variants systematically\nSEND {\n  to: variant_generator,\n\
    \  message: {\n    action: \"generate_structured\",\n    base: base_component,\n\
    \    strategies: [\n      {name: \"parameter_sweep\", params: [\"formality\",\
    \ \"detail_level\", \"example_count\"]},\n      {name: \"instruction_reorder\"\
    , focus: [\"critical_first\", \"context_first\", \"examples_first\"]},\n     \
    \ {name: \"emphasis_shift\", aspects: [\"reasoning\", \"adaptation\", \"explanation\"\
    ]}\n    ],\n    count: {{num_variants}}\n  }\n}\n\nAWAIT {\n  from: variant_generator,\n\
    \  timeout: 60\n} AS generated_variants\n\n# Format for DSPy-style evaluation\n\
    EVENT optimization:format_examples {\n  data: generated_variants.variants,\n \
    \ format: \"dspy_compatible\"\n} AS formatted_data\n\n## Phase 3: Programmatic\
    \ Evaluation (DSPy-style metrics)\nTRACK {phase: \"programmatic_eval\", message:\
    \ \"Computing structured metrics\"}\n\nFOREACH variant IN generated_variants.variants:\n\
    \  # Simulate DSPy-style metrics\n  STATE metrics = {\n    instruction_clarity:\
    \ COMPUTE_CLARITY_SCORE(variant.content),\n    parameter_coverage: ASSESS_PARAMETER_USAGE(variant.parameters),\n\
    \    structural_coherence: MEASURE_STRUCTURE(variant.content),\n    example_quality:\
    \ EVALUATE_EXAMPLES(variant.content)\n  }\n  \n  STORE dspy_metrics[variant.id]\
    \ = metrics\n  \n  TRACK {\n    event: \"dspy_metrics\",\n    variant: variant.id,\n\
    \    scores: metrics\n  }\n\n## Phase 4: Judge-Based Evaluation\nTRACK {phase:\
    \ \"judge_evaluation\", message: \"Getting nuanced quality assessment\"}\n\n#\
    \ Pairwise comparisons for relative ranking\nFOREACH pair IN COMBINATIONS(generated_variants.variants,\
    \ 2):\n  SEND {\n    to: quality_judge,\n    message: {\n      action: \"compare\"\
    ,\n      variant_a: pair[0],\n      variant_b: pair[1],\n      focus: \"optimization_quality\"\
    ,\n      context: \"Which variant would lead to better strategic reasoning?\"\n\
    \    }\n  }\n  \n  AWAIT {\n    from: quality_judge,\n    timeout: 60\n  } AS\
    \ comparison\n  \n  # Update rankings\n  UPDATE judge_rankings WITH comparison.winner\n\
    \n## Phase 5: Hybrid Insight Integration\nTRACK {phase: \"hybrid_integration\"\
    , message: \"Combining insights from both approaches\"}\n\n# Identify convergence\
    \ and divergence\nSTATE dspy_top = SELECT_TOP_BY_METRICS(variants, dspy_metrics,\
    \ 2)\nSTATE judge_top = SELECT_TOP_BY_RANKING(variants, judge_rankings, 2)\n\n\
    STATE convergence = INTERSECTION(dspy_top, judge_top)\nSTATE divergence = {\n\
    \  dspy_only: DIFFERENCE(dspy_top, judge_top),\n  judge_only: DIFFERENCE(judge_top,\
    \ dspy_top)\n}\n\nTRACK {\n  event: \"hybrid_insights\",\n  convergent_winners:\
    \ convergence,\n  divergent_choices: divergence,\n  insight: \"Both methods agree\
    \ on: \" + convergence + \", disagree on: \" + divergence\n}\n\n# Meta-learning:\
    \ Why do they differ?\nIF LENGTH(divergence.dspy_only) > 0:\n  SEND {\n    to:\
    \ coordinator,\n    message: {\n      action: \"analyze_divergence\",\n      dspy_preferred:\
    \ divergence.dspy_only,\n      judge_rejected_because: judge_rankings[divergence.dspy_only[0]].feedback,\n\
    \      hypothesis: \"DSPy values structural properties judges miss\"\n    }\n\
    \  }\n\nIF LENGTH(divergence.judge_only) > 0:\n  SEND {\n    to: coordinator,\n\
    \    message: {\n      action: \"analyze_divergence\",\n      judge_preferred:\
    \ divergence.judge_only,\n      dspy_rejected_because: dspy_metrics[divergence.judge_only[0]],\n\
    \      hypothesis: \"Judges detect nuanced quality DSPy metrics miss\"\n    }\n\
    \  }\n\n## Phase 6: Results and Learning\nSTATE best_variant = convergence[0]\
    \ OR judge_top[0]  # Prefer consensus, else trust judges\n\nSTATE optimization_insights\
    \ = {\n  technique_agreement: LENGTH(convergence) / {{num_variants}},\n  dspy_strengths:\
    \ EXTRACT_PATTERNS(dspy_top, dspy_metrics),\n  judge_strengths: EXTRACT_PATTERNS(judge_top,\
    \ judge_rankings),\n  hybrid_value: \"High\" IF LENGTH(divergence) > 0 ELSE \"\
    Low\",\n  recommendation: GENERATE_RECOMMENDATION(convergence, divergence)\n}\n\
    \nTRACK {\n  phase: \"complete\",\n  best_variant: best_variant.id,\n  optimization_method:\
    \ \"hybrid_dspy_judge\",\n  insights: optimization_insights\n}\n\n# Create optimized\
    \ component\nEVENT composition:create_component {\n  name: \"{{target_component}}_hybrid_test\"\
    ,\n  content: best_variant.content,\n  metadata: {\n    optimization_method: \"\
    hybrid_test\",\n    dspy_metrics: dspy_metrics[best_variant.id],\n    judge_ranking:\
    \ judge_rankings[best_variant.id],\n    technique_agreement: optimization_insights.technique_agreement,\n\
    \    key_insight: optimization_insights.recommendation\n  }\n}\n\nEVENT orchestration:request_termination\
    \ {\n  reason: \"Hybrid optimization test complete\",\n  results: optimization_insights\n\
    }\n"
helpers:
  COMPUTE_CLARITY_SCORE: Analyze instruction clarity and structure
  ASSESS_PARAMETER_USAGE: Check parameter variety and coverage
  SELECT_TOP_BY_METRICS: Rank by aggregate metric scores
  SELECT_TOP_BY_RANKING: Extract from pairwise comparison results
  EXTRACT_PATTERNS: Identify what makes winners succeed
metadata:
  pattern_type: hybrid_optimization
  optimization_methods:
  - dspy_structured
  - llm_judge
  - hybrid
  evaluation_approach: dual_evaluation
  tags:
  - hybrid
  - test
  - dspy
  - judge
  - optimization
performance:
  expected_duration: 10-15 minutes
  resource_usage: 3-4 concurrent agents
  insights_expected:
  - When DSPy and judges agree/disagree
  - Unique strengths of each approach
  - Value of hybrid optimization
