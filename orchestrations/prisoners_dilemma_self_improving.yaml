name: prisoners_dilemma_self_improving
type: orchestration
version: 1.1.0
description: |
  Enhanced Prisoner's Dilemma that triggers optimization of player strategies
  when cooperation rates fall below threshold. A simple example of self-improving
  orchestration built on proven patterns.
author: ksi_system
timestamp: 2025-07-23T21:00:00Z

# Inherit most structure from original
extends: prisoners_dilemma_judge_evaluation

# Additional variables for self-improvement
variables:
  num_rounds: "{{num_rounds|default:20}}"
  cooperation_threshold: "{{cooperation_threshold|default:0.6}}"  # Trigger optimization below this
  optimization_enabled: "{{optimization_enabled|default:true}}"
  
  # Models for different roles
  player_model: "claude-cli/sonnet"
  judge_model: "claude-cli/sonnet"

# Add learning tracker agent
agents:
  # Inherit player_a, player_b, strategy_judge from parent
  
  # New: Performance Monitor
  performance_monitor:
    component: "components/core/base_agent"
    vars:
      agent_id: "performance_monitor"
      prompt: |
        You monitor game outcomes and trigger optimization when needed.
        
        ## MANDATORY: Start with:
        {"event": "agent:status", "data": {"agent_id": "{{agent_id}}", "status": "monitor_initialized"}}
        
        Track cooperation rates and strategy effectiveness.
        When cooperation falls below {{cooperation_threshold}}, recommend optimization.

orchestration_logic:
  strategy: |
    ## Phases 1-3: Inherited from parent (Initialize, Play Game, Judge Evaluation)
    {{inherit:phases_1_to_3}}
    
    ## Phase 4: Performance Analysis & Self-Improvement
    STATE cooperation_rate = CALCULATE_COOPERATION_RATE(game_history)
    STATE mutual_cooperation_rounds = COUNT(game_history, outcome == "mutual_cooperation")
    STATE defection_spirals = COUNT_DEFECTION_SPIRALS(game_history)
    
    TRACK {
      phase: "performance_analysis",
      cooperation_rate: cooperation_rate,
      mutual_cooperation: mutual_cooperation_rounds,
      defection_spirals: defection_spirals,
      threshold: {{cooperation_threshold}}
    }
    
    # Store game results for learning
    EVENT state:entity:create {
      type: "game_result",
      id: "pd_result_{{TIMESTAMP()}}",
      properties: {
        cooperation_rate: cooperation_rate,
        strategies: {
          player_a: "adaptive_cooperator",
          player_b: "tit_for_tat"
        },
        game_history: game_history,
        insights: EXTRACT_STRATEGIC_INSIGHTS(game_history)
      }
    }
    
    ## Phase 5: Conditional Optimization
    IF {{optimization_enabled}} AND cooperation_rate < {{cooperation_threshold}}:
      TRACK {
        event: "optimization_triggered",
        reason: "low_cooperation",
        current_rate: cooperation_rate,
        target_rate: {{cooperation_threshold}}
      }
      
      # Analyze which strategy needs improvement
      STATE player_a_defections = COUNT(game_history, player_a.action == "defect")
      STATE player_b_defections = COUNT(game_history, player_b.action == "defect")
      
      STATE optimization_target = null
      STATE optimization_objective = ""
      
      IF player_a_defections > player_b_defections * 1.5:
        STATE optimization_target = "components/personas/game_players/adaptive_cooperator"
        STATE optimization_objective = "Reduce defection rate while maintaining strategic flexibility. Improve cooperation emergence and forgiveness mechanisms."
      ELIF player_b_defections > player_a_defections * 1.5:
        STATE optimization_target = "components/personas/game_players/tit_for_tat"
        STATE optimization_objective = "Enhance forgiveness probability and add noise tolerance. Reduce retaliation cycles."
      ELSE:
        # Both need work - optimize the one with lower individual score
        STATE optimization_target = player_a_score < player_b_score ? 
          "components/personas/game_players/adaptive_cooperator" : 
          "components/personas/game_players/tit_for_tat"
        STATE optimization_objective = "Improve ability to establish and maintain mutual cooperation. Break defection spirals more effectively."
      
      # Trigger optimization
      EVENT orchestration:start {
        pattern: "orchestrations/simple_component_optimization",
        vars: {
          target_component: optimization_target,
          optimization_objective: optimization_objective,
          max_trials: 5,
          evaluation_metric: "cooperation_emergence"
        }
      } AS optimization_job
      
      TRACK {
        event: "optimization_started",
        target: optimization_target,
        objective: optimization_objective,
        optimization_id: optimization_job.orchestration_id
      }
      
      # Create learning record
      EVENT state:entity:create {
        type: "pd_optimization_trigger",
        id: "pd_opt_{{TIMESTAMP()}}",
        properties: {
          game_result_id: "pd_result_{{TIMESTAMP()}}",
          cooperation_rate: cooperation_rate,
          optimization_target: optimization_target,
          optimization_job: optimization_job.orchestration_id,
          hypothesis: "Optimizing " + optimization_target + " will improve cooperation"
        }
      }
    
    ELSE:
      TRACK {
        event: "optimization_not_needed",
        cooperation_rate: cooperation_rate,
        reason: cooperation_rate >= {{cooperation_threshold}} ? "threshold_met" : "optimization_disabled"
      }
    
    ## Phase 6: Meta-Learning
    # Query past optimizations to learn what works
    EVENT state:entity:query {
      type: "pd_optimization_trigger",
      limit: 10,
      sort: {timestamp: -1}
    } AS past_optimizations
    
    IF LENGTH(past_optimizations) > 3:
      STATE optimization_patterns = ANALYZE_OPTIMIZATION_OUTCOMES(past_optimizations)
      
      TRACK {
        event: "meta_learning",
        patterns_found: optimization_patterns.successful_strategies,
        avg_improvement: optimization_patterns.average_cooperation_gain,
        best_strategy: optimization_patterns.most_effective_optimization
      }
      
      # Store meta-insights
      EVENT state:entity:create {
        type: "pd_meta_insights",
        id: "pd_meta_{{TIMESTAMP()}}",
        properties: {
          total_games_analyzed: LENGTH(past_optimizations),
          successful_optimizations: optimization_patterns.success_count,
          key_insights: optimization_patterns.insights,
          recommended_defaults: optimization_patterns.recommended_strategies
        }
      }
    
    ## Phase 7: Report & Termination
    STATE final_report = {
      game_outcome: {
        final_scores: {a: player_a_score, b: player_b_score},
        cooperation_rate: cooperation_rate,
        winner: player_a_score > player_b_score ? "player_a" : "player_b"
      },
      optimization: {
        triggered: optimization_target != null,
        target: optimization_target,
        reason: optimization_objective
      },
      learning: {
        game_recorded: true,
        past_games_analyzed: LENGTH(past_optimizations),
        meta_insights_generated: LENGTH(past_optimizations) > 3
      }
    }
    
    TRACK {
      event: "pd_complete",
      report: final_report
    }
    
    EVENT orchestration:request_termination {
      reason: "Prisoner's Dilemma complete with learning",
      cooperation_achieved: cooperation_rate,
      optimization_triggered: optimization_target != null,
      report: final_report
    }

# Helper functions
helpers:
  CALCULATE_COOPERATION_RATE: |
    Count rounds where both players cooperated divided by total rounds
  COUNT_DEFECTION_SPIRALS: |
    Count sequences of 3+ consecutive mutual defections
  EXTRACT_STRATEGIC_INSIGHTS: |
    Identify key decision points, strategy shifts, and cooperation breakdowns
  ANALYZE_OPTIMIZATION_OUTCOMES: |
    Compare cooperation rates before/after optimization across games

metadata:
  pattern_type: self_improving_game
  optimization_integration: simple
  learning_mechanism: state_based
  parent_pattern: prisoners_dilemma_judge_evaluation
  tags: ["game_theory", "self_improvement", "optimization", "learning"]

performance:
  expected_duration: "5-10 minutes for game, +15 minutes if optimization triggered"
  resource_usage: "4 agents for game, +1 orchestration if optimizing"
  improvement_expected: "10-30% cooperation rate increase after optimization"