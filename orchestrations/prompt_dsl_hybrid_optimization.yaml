name: prompt_dsl_hybrid_optimization
type: orchestration
version: 1.0.0
description: |
  MIPRO optimization that explores how DSL elements can enhance natural language prompts.
  Tests the hypothesis that structured DSL patterns improve prompt reliability and interpretability.
author: ksi_system
timestamp: 2025-01-18T18:30:00Z

# Define agents
agents:
  # Hybrid Prompt Optimizer
  hybrid_optimizer:
    component: "components/core/system_orchestrator"
    vars:
      agent_id: "hybrid_optimizer"
      prompt: |
        You optimize prompts by blending natural language with DSL elements.
        
        ## MANDATORY: Start with:
        {"event": "agent:status", "data": {"agent_id": "{{agent_id}}", "status": "hybrid_optimizer_initialized", "domain": "{{domain}}"}}
        
        Your approach:
        1. Start with pure natural language prompts
        2. Gradually introduce DSL constructs where they add clarity
        3. Find the optimal balance between structure and naturalness
        4. Test different integration patterns
        
        DSL elements to consider:
        - Structured conditions: IF/THEN clarity
        - State tracking: STATE declarations
        - Clear sequencing: FIRST, THEN, FINALLY
        - Event patterns: WHEN X OCCURS
        - Data operations: EXTRACT, FILTER, AGGREGATE

  # Prompt Variant Generator
  prompt_dsl_mixer:
    component: "components/personas/creative_thinker"
    vars:
      agent_id: "pattern_mixer"
      prompt_suffix: |
        
        Create prompt variants that blend natural language with DSL elements.
        
        Start from a base prompt and systematically add structure:
        1. Pure natural language baseline
        2. Add clear section markers (##, ###)
        3. Introduce conditional logic (IF/THEN)
        4. Add state tracking (STATE, TRACK)
        5. Include structured outputs (EVENT, EMIT)
        
        Find sweet spots where structure enhances rather than constrains.

  # Prompt Effectiveness Tester
  prompt_tester:
    component: "components/core/base_agent"
    vars:
      agent_id: "pattern_tester"
      prompt: |
        You test prompt variants on specific tasks.
        
        ## MANDATORY: Start with:
        {"event": "agent:status", "data": {"agent_id": "{{agent_id}}", "status": "tester_ready"}}
        
        For each prompt variant:
        1. Execute the task following the prompt
        2. Measure adherence to instructions
        3. Check output structure compliance
        4. Note any confusion or ambiguity
        5. Rate effectiveness (1-10)
        
        ## MANDATORY: Report test results:
        {"event": "state:entity:create", "data": {"type": "prompt_test_result", "id": "test_{{variant_id}}", "properties": {"effectiveness": N, "structure_compliance": M, "ambiguities": []}}}

  # Hybrid Pattern Judge
  pattern_judge:
    component: "components/personas/judges/optimization_technique_judge"
    vars:
      agent_id: "pattern_judge"
      prompt_suffix: |
        
        Evaluate prompt+DSL hybrid patterns based on:
        1. Clarity improvement over pure natural language
        2. Ease of following structured instructions
        3. Flexibility vs rigidity balance
        4. Cognitive load on the LLM
        5. Robustness across different tasks

# Configuration
variables:
  domain: "{{domain|default:task_decomposition}}"  # task_decomposition, decision_making, data_analysis, etc.
  base_prompt: "{{base_prompt}}"
  num_iterations: "{{num_iterations|default:10}}"
  dsl_integration_levels: "{{levels|default:5}}"  # How many levels of DSL integration to test

orchestration_logic:
  strategy: |
    ## Phase 1: Initialize Hybrid Optimization
    STATE prompt_variants = []
    STATE test_results = {}
    STATE optimal_patterns = []
    STATE integration_sweet_spot = null
    
    TRACK {
      phase: "hybrid_init",
      domain: "{{domain}}",
      optimization_goal: "find optimal natural language + DSL blend"
    }
    
    # Define test tasks for the domain
    STATE test_tasks = GENERATE_DOMAIN_TASKS("{{domain}}", 5)
    
    ## Phase 2: Baseline Testing
    TRACK {phase: "baseline", message: "Testing pure natural language prompt"}
    
    # Create pure natural language baseline
    STATE baseline_prompt = "{{base_prompt}}" OR GENERATE_BASELINE("{{domain}}")
    
    APPEND prompt_variants {
      id: "baseline",
      content: baseline_prompt,
      dsl_level: 0,
      dsl_elements: []
    }
    
    # Test baseline
    FOREACH task IN test_tasks:
      SEND {
        to: prompt_tester,
        message: {
          action: "test_prompt",
          prompt: baseline_prompt,
          task: task,
          variant_id: "baseline"
        }
      }
      
      AWAIT {
        from: prompt_tester,
        timeout: 90
      } AS baseline_result
      
      APPEND test_results["baseline"] baseline_result
    
    STATE baseline_score = AVERAGE(test_results["baseline"], r.properties.effectiveness)
    
    ## Phase 3: Progressive DSL Integration
    LOOP level FROM 1 TO {{dsl_integration_levels}}:
      TRACK {
        phase: "dsl_integration",
        level: level,
        testing_variants: LENGTH(prompt_variants)
      }
      
      # Generate variants with increasing DSL integration
      SEND {
        to: prompt_dsl_mixer,
        message: {
          action: "create_hybrid",
          base_prompt: baseline_prompt,
          integration_level: level,
          successful_patterns: optimal_patterns,
          domain: "{{domain}}",
          focus_elements: SELECT_DSL_ELEMENTS(level)
        }
      }
      
      AWAIT {
        from: prompt_dsl_mixer,
        timeout: 120
      } AS hybrid_variants
      
      # Test each hybrid variant
      FOREACH variant IN hybrid_variants.variants:
        STATE variant_id = "hybrid_L{{level}}_{{INDEX(variant)}}"
        
        APPEND prompt_variants {
          id: variant_id,
          content: variant.prompt,
          dsl_level: level,
          dsl_elements: variant.elements_used,
          integration_strategy: variant.strategy
        }
        
        # Run tests
        STATE variant_scores = []
        STATE structure_compliance = []
        
        FOREACH task IN test_tasks:
          SEND {
            to: prompt_tester,
            message: {
              action: "test_prompt",
              prompt: variant.prompt,
              task: task,
              variant_id: variant_id,
              expected_structure: variant.expected_outputs
            }
          }
          
          AWAIT {
            from: prompt_tester,
            timeout: 90
          } AS test_result
          
          APPEND variant_scores test_result.properties.effectiveness
          APPEND structure_compliance test_result.properties.structure_compliance
          
          # Track particularly effective patterns
          IF test_result.properties.effectiveness >= 8:
            ANALYZE_SUCCESS(variant, test_result) AS success_pattern
            IF success_pattern.is_novel:
              APPEND optimal_patterns success_pattern
        
        # Calculate variant performance
        STATE avg_effectiveness = MEAN(variant_scores)
        STATE avg_compliance = MEAN(structure_compliance)
        STATE combined_score = 0.7 * avg_effectiveness + 0.3 * avg_compliance
        
        UPDATE prompt_variants WHERE id == variant_id:
          effectiveness_score = avg_effectiveness
          compliance_score = avg_compliance
          combined_score = combined_score
      
      # Analyze level performance
      STATE level_variants = FILTER(prompt_variants, v.dsl_level == level)
      STATE level_avg_score = MEAN(level_variants, v.combined_score)
      
      TRACK {
        event: "level_complete",
        level: level,
        avg_score: level_avg_score,
        best_variant: MAX(level_variants, v.combined_score),
        improvement_over_baseline: level_avg_score - baseline_score
      }
      
      # Early stopping if performance degrades
      IF level > 2 AND level_avg_score < PREVIOUS_LEVEL_SCORE * 0.95:
        TRACK {
          event: "early_stopping",
          reason: "performance_degradation",
          optimal_level: level - 1
        }
        SET integration_sweet_spot = level - 1
        BREAK
    
    ## Phase 4: Pattern Analysis
    TRACK {phase: "pattern_analysis", message: "Identifying optimal DSL integration patterns"}
    
    # Group variants by DSL elements used
    STATE element_effectiveness = {}
    FOREACH element IN ALL_DSL_ELEMENTS:
      STATE variants_with_element = FILTER(prompt_variants, element IN v.dsl_elements)
      IF LENGTH(variants_with_element) > 0:
        SET element_effectiveness[element] = {
          avg_score: MEAN(variants_with_element, v.combined_score),
          usage_count: LENGTH(variants_with_element),
          best_context: ANALYZE_CONTEXT(variants_with_element)
        }
    
    # Find synergistic combinations
    STATE element_combinations = ANALYZE_COMBINATIONS(prompt_variants)
    STATE synergistic_patterns = FILTER(element_combinations, c.synergy_score > 1.2)
    
    ## Phase 5: Optimization Synthesis
    TRACK {phase: "synthesis", message: "Creating optimized prompt template"}
    
    # Select best variants for final comparison
    STATE finalists = SELECT_TOP(prompt_variants, 5, by="combined_score")
    
    SEND {
      to: pattern_judge,
      message: {
        action: "evaluate_patterns",
        variants: finalists,
        baseline: baseline_prompt,
        element_analysis: element_effectiveness,
        synergistic_patterns: synergistic_patterns
      }
    }
    
    AWAIT {
      from: pattern_judge,
      timeout: 180
    } AS final_evaluation
    
    # Create optimized prompt template
    STATE optimized_template = {
      domain: "{{domain}}",
      base_structure: final_evaluation.recommended_structure,
      mandatory_elements: final_evaluation.high_impact_elements,
      optional_elements: final_evaluation.contextual_elements,
      integration_level: integration_sweet_spot OR final_evaluation.optimal_level,
      usage_guidelines: final_evaluation.guidelines
    }
    
    # Document the optimization
    EVENT composition:create_component {
      name: "prompt_templates/{{domain}}_dsl_enhanced",
      content: GENERATE_PROMPT_TEMPLATE(optimized_template),
      metadata: {
        optimization_method: "prompt_dsl_hybrid",
        domain: "{{domain}}",
        effectiveness_gain: MAX(finalists, f.combined_score) - baseline_score,
        dsl_elements: optimized_template.mandatory_elements,
        tested_variants: LENGTH(prompt_variants)
      }
    }
    
    # Create pattern library
    EVENT composition:create_component {
      name: "components/behaviors/dsl_patterns/{{domain}}_patterns",
      content: GENERATE_PATTERN_LIBRARY(optimal_patterns, element_effectiveness),
      metadata: {
        pattern_count: LENGTH(optimal_patterns),
        top_elements: SELECT_TOP(element_effectiveness, 5, by="avg_score"),
        synergistic_combinations: synergistic_patterns
      }
    }
    
    TRACK {
      phase: "complete",
      optimization_summary: {
        domain: "{{domain}}",
        variants_tested: LENGTH(prompt_variants),
        optimal_dsl_level: integration_sweet_spot,
        best_score: MAX(prompt_variants, v.combined_score),
        improvement: MAX(prompt_variants, v.combined_score) - baseline_score,
        winning_elements: optimized_template.mandatory_elements
      }
    }
    
    EVENT orchestration:request_termination {
      reason: "Prompt+DSL hybrid optimization complete",
      results: {
        domain: "{{domain}}",
        effectiveness_improvement: (MAX(finalists, f.combined_score) - baseline_score) / baseline_score,
        optimal_integration_level: integration_sweet_spot,
        discovered_patterns: LENGTH(optimal_patterns)
      }
    }

# Helper functions
helpers:
  SELECT_DSL_ELEMENTS: |
    # Progressive introduction of DSL elements
    # Level 1: Section markers (##, ###)
    # Level 2: Simple conditions (IF/THEN)
    # Level 3: State tracking (STATE, TRACK)
    # Level 4: Structured I/O (EVENT, EMIT)
    # Level 5: Full orchestration patterns
    
  ANALYZE_SUCCESS: |
    # Extract what made a variant successful
    # - Which DSL elements contributed most
    # - How they improved task execution
    # - Context where they excel
    
  GENERATE_PROMPT_TEMPLATE: |
    # Create reusable prompt template with:
    # - Clear structure
    # - Placeholder sections
    # - DSL integration points
    # - Usage examples

# Example DSL elements for integration
dsl_elements:
  structural:
    - "## MANDATORY: [instruction]"
    - "### Phase N: [phase_name]"
    - "WHEN [condition]: [action]"
  
  conditional:
    - "IF [condition]: [action] ELSE: [alternative]"
    - "FOREACH [item] IN [collection]: [process]"
  
  state_tracking:
    - "STATE [variable] = [value]"
    - "TRACK {event: '[name]', data: {...}}"
  
  output_structure:
    - "EMIT '[event_name]' WITH: {...}"
    - '{"event": "[name]", "data": {...}}'

# Metadata
metadata:
  pattern_type: hybrid_optimization
  optimization_target: prompt_dsl_integration
  evaluation_method: task_based_testing
  capabilities_demonstrated:
    - prompt_engineering
    - dsl_integration
    - pattern_discovery
    - effectiveness_measurement
  tags: ["prompt_optimization", "dsl_hybrid", "mipro", "structured_prompting"]

# Performance expectations
performance:
  expected_duration: "45-60 minutes"
  resource_usage: "4-5 concurrent agents"
  success_metrics:
    effectiveness_improvement: "> 25% over baseline"
    optimal_integration: "Level 2-3 typical"
    pattern_reusability: "High across similar domains"