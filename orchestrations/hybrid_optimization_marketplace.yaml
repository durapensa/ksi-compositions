name: hybrid_optimization_marketplace
type: orchestration
version: 1.0.0
description: |
  Meta-optimization framework that compares and combines different optimization techniques.
  Implements an "optimization marketplace" where DSPy, LLM-as-Judge, and hybrid approaches
  compete during bootstrapping to discover the best technique for each use case.
author: ksi_system
timestamp: 2025-01-18T14:00:00Z

# Define optimization agents
agents:
  # Meta-Optimization Coordinator
  meta_coordinator:
    component: "components/core/system_orchestrator"
    vars:
      agent_id: "meta_coordinator"
      prompt: |
        You coordinate a marketplace of optimization techniques.
        
        ## MANDATORY: Start with:
        {"event": "agent:status", "data": {"agent_id": "{{agent_id}}", "status": "meta_coordinator_initialized", "target": "{{target_component}}"}}
        
        Your role:
        1. Run multiple optimization techniques in parallel
        2. Compare their effectiveness empirically
        3. Select the best approach for production optimization
        4. Learn patterns about which techniques excel where
        
        ## MANDATORY: Track marketplace state:
        {"event": "state:entity:create", "data": {"type": "marketplace_state", "id": "{{agent_id}}_market", "properties": {"techniques": ["dspy", "judge", "hybrid"], "phase": "comparison"}}}

  # DSPy Optimization Agent
  dspy_optimizer:
    component: "components/core/system_single_agent"
    vars:
      agent_id: "dspy_optimizer"
      prompt: |
        You coordinate DSPy/MIPRO optimization through the KSI optimization service.
        
        ## MANDATORY: Start with:
        {"event": "agent:status", "data": {"agent_id": "{{agent_id}}", "status": "dspy_optimizer_initialized"}}
        
        Use these optimization events:
        - optimization:format_examples - Prepare training data
        - optimization:get_framework_info - Query DSPy capabilities
        - optimization:validate_setup - Check configuration
        
        Focus on:
        1. Systematic prompt variations using DSPy's algorithms
        2. Efficient bootstrapping with programmatic metrics
        3. Bayesian optimization for parameter search

  # Judge-Based Optimizer
  judge_optimizer:
    component: "components/core/system_orchestrator"
    vars:
      agent_id: "judge_optimizer"
      prompt: |
        You run judge-based optimization using pairwise comparisons.
        
        ## MANDATORY: Start with:
        {"event": "agent:status", "data": {"agent_id": "{{agent_id}}", "status": "judge_optimizer_initialized"}}
        
        Your approach:
        1. Generate variants through creative exploration
        2. Use pairwise judge comparisons for evaluation
        3. Build Elo ratings from relative rankings
        4. Extract patterns from judge feedback

  # Hybrid Optimizer
  hybrid_optimizer:
    component: "components/core/system_orchestrator"
    vars:
      agent_id: "hybrid_optimizer"
      prompt: |
        You combine DSPy search with judge evaluation.
        
        ## MANDATORY: Start with:
        {"event": "agent:status", "data": {"agent_id": "{{agent_id}}", "status": "hybrid_optimizer_initialized"}}
        
        Hybrid strategy:
        1. Use DSPy for structured variant generation
        2. Evaluate candidates with LLM judges
        3. Feed judge insights back to DSPy
        4. Iterate with both programmatic and semantic feedback

  # Technique Evaluator
  technique_evaluator:
    component: "components/personas/judges/optimization_technique_judge"
    vars:
      agent_id: "technique_evaluator"
      prompt_suffix: |
        
        Compare optimization techniques based on:
        - Quality of variants produced
        - Speed of convergence
        - Diversity of solutions
        - Robustness across domains

# Configuration
variables:
  target_component: "{{target_component}}"
  optimization_domain: "{{domain|default:'general'}}"  # game_theory, code_gen, creative, etc.
  bootstrap_iterations: "{{bootstrap_iterations|default:5}}"
  
  # Technique configurations
  enable_dspy: "{{enable_dspy|default:true}}"
  enable_judge: "{{enable_judge|default:true}}"
  enable_hybrid: "{{enable_hybrid|default:true}}"

orchestration_logic:
  strategy: |
    ## Phase 1: Initialize Marketplace
    STATE techniques = []
    STATE technique_results = {}
    STATE bootstrap_data = []
    STATE domain_insights = {}
    
    TRACK {
      phase: "marketplace_init",
      target: "{{target_component}}",
      domain: "{{optimization_domain}}",
      enabled_techniques: {
        dspy: {{enable_dspy}},
        judge: {{enable_judge}},
        hybrid: {{enable_hybrid}}
      }
    }
    
    # Load target component
    EVENT composition:get_component {
      name: "{{target_component}}"
    } AS base_component
    
    ## Phase 2: Bootstrap Competition
    TRACK {phase: "bootstrap_competition", message: "Running parallel optimization techniques"}
    
    # Prepare shared bootstrap data
    STATE bootstrap_tasks = GENERATE_TASKS({{optimization_domain}}, 10)
    
    # Launch optimizers in parallel
    STATE active_optimizers = []
    
    IF {{enable_dspy}}:
      SPAWN dspy_optimizer
      APPEND active_optimizers "dspy"
      
      SEND {
        to: dspy_optimizer,
        message: {
          action: "bootstrap",
          component: base_component,
          tasks: bootstrap_tasks,
          iterations: {{bootstrap_iterations}},
          use_events: ["optimization:format_examples", "optimization:validate_setup"]
        }
      }
    
    IF {{enable_judge}}:
      SPAWN judge_optimizer
      APPEND active_optimizers "judge"
      
      SEND {
        to: judge_optimizer,
        message: {
          action: "bootstrap",
          component: base_component,
          tasks: bootstrap_tasks,
          iterations: {{bootstrap_iterations}},
          judge_type: "pairwise_comparison"
        }
      }
    
    IF {{enable_hybrid}}:
      SPAWN hybrid_optimizer
      APPEND active_optimizers "hybrid"
      
      SEND {
        to: hybrid_optimizer,
        message: {
          action: "bootstrap",
          component: base_component,
          tasks: bootstrap_tasks,
          iterations: {{bootstrap_iterations}},
          strategy: "dspy_search_judge_eval"
        }
      }
    
    # Collect bootstrap results
    AWAIT {
      from: active_optimizers,
      event_pattern: "optimization:bootstrap_complete",
      timeout: 300,
      count: LENGTH(active_optimizers)
    } AS bootstrap_results
    
    ## Phase 3: Empirical Comparison
    TRACK {phase: "technique_comparison", message: "Evaluating optimization approaches"}
    
    # Extract top variants from each technique
    FOREACH result IN bootstrap_results:
      STATE technique_name = result.technique
      STATE top_variants = SELECT_TOP(result.variants, 3)
      
      STORE technique_results[technique_name] = {
        variants: top_variants,
        metrics: result.metrics,
        convergence_rate: result.convergence_rate,
        diversity_score: CALCULATE_DIVERSITY(top_variants)
      }
    
    # Cross-evaluate variants
    STATE cross_eval_results = []
    
    FOREACH tech_a, data_a IN technique_results:
      FOREACH tech_b, data_b IN technique_results:
        IF tech_a != tech_b:
          # Compare best variants from each technique
          SEND {
            to: technique_evaluator,
            message: {
              action: "compare_variants",
              variant_a: data_a.variants[0],
              variant_b: data_b.variants[0],
              source_a: tech_a,
              source_b: tech_b,
              evaluation_tasks: SAMPLE(bootstrap_tasks, 3)
            }
          }
          
          AWAIT {
            from: technique_evaluator,
            timeout: 60
          } AS comparison
          
          APPEND cross_eval_results comparison
    
    # Analyze technique performance
    STATE technique_rankings = COMPUTE_RANKINGS(cross_eval_results)
    STATE domain_winner = SELECT_BEST(technique_rankings)
    
    TRACK {
      event: "bootstrap_winner",
      domain: "{{optimization_domain}}",
      winner: domain_winner,
      rankings: technique_rankings,
      insights: EXTRACT_INSIGHTS(cross_eval_results)
    }
    
    ## Phase 4: Production Optimization
    TRACK {phase: "production_optimization", selected_technique: domain_winner}
    
    # Run full optimization with winning technique
    IF domain_winner == "dspy":
      SEND {
        to: dspy_optimizer,
        message: {
          action: "optimize_full",
          component: base_component,
          config: {
            max_iterations: 20,
            use_mipro_v2: true,
            metric_threshold: 0.8
          }
        }
      }
    ELIF domain_winner == "judge":
      SEND {
        to: judge_optimizer,
        message: {
          action: "optimize_full",
          component: base_component,
          config: {
            max_iterations: 15,
            variants_per_iteration: 4,
            judge_ensemble_size: 3
          }
        }
      }
    ELSE:  # hybrid
      SEND {
        to: hybrid_optimizer,
        message: {
          action: "optimize_full",
          component: base_component,
          config: {
            dspy_iterations: 10,
            judge_refinements: 5,
            feedback_integration: "bidirectional"
          }
        }
      }
    
    AWAIT {
      from: domain_winner + "_optimizer",
      event_pattern: "optimization:complete",
      timeout: 600
    } AS final_result
    
    ## Phase 5: Meta-Learning
    TRACK {phase: "meta_learning", message: "Updating optimization knowledge"}
    
    # Record domain-technique mapping
    STATE meta_insight = {
      domain: "{{optimization_domain}}",
      component_type: base_component.frontmatter.component_type,
      winning_technique: domain_winner,
      technique_scores: technique_rankings,
      key_factors: ANALYZE_SUCCESS_FACTORS(cross_eval_results, domain_winner),
      timestamp: NOW()
    }
    
    # Update meta-optimization knowledge base
    EVENT state:entity:create {
      type: "optimization_insight",
      id: "optimization_insight",
      properties: meta_insight
    }
    
    # Create optimized component
    EVENT composition:create_component {
      name: "{{target_component}}_{{domain_winner}}_optimized",
      content: final_result.best_variant.content,
      metadata: {
        optimization_method: domain_winner,
        original_component: "{{target_component}}",
        domain: "{{optimization_domain}}",
        performance_gain: final_result.improvement,
        technique_comparison: technique_rankings,
        meta_insights: meta_insight.key_factors
      }
    }
    
    # Generate marketplace report
    STATE marketplace_summary = {
      techniques_tested: active_optimizers,
      bootstrap_winner: domain_winner,
      production_results: final_result.metrics,
      cross_evaluation_matrix: BUILD_MATRIX(cross_eval_results),
      recommendations: {
        domain: "{{optimization_domain}}",
        preferred_technique: domain_winner,
        secondary_technique: SELECT_SECOND(technique_rankings),
        hybrid_potential: ASSESS_HYBRID_VALUE(technique_results)
      }
    }
    
    TRACK {
      phase: "complete",
      marketplace_summary: marketplace_summary,
      optimized_component: "{{target_component}}_{{domain_winner}}_optimized"
    }
    
    # Request termination
    EVENT orchestration:request_termination {
      reason: "Hybrid optimization marketplace complete",
      results: {
        selected_technique: domain_winner,
        performance_comparison: technique_rankings,
        meta_learning: meta_insight
      }
    }

# Helper functions
helpers:
  GENERATE_TASKS: |
    # Generate domain-appropriate evaluation tasks
    # game_theory → strategic scenarios
    # code_gen → programming challenges
    # creative → writing prompts
    
  CALCULATE_DIVERSITY: |
    # Measure semantic diversity of variants
    # Higher diversity → more exploration
    
  COMPUTE_RANKINGS: |
    # Aggregate pairwise comparisons into rankings
    # Use Bradley-Terry or similar model
    
  EXTRACT_INSIGHTS: |
    # Identify why certain techniques excel
    # Pattern recognition across comparisons
    
  ANALYZE_SUCCESS_FACTORS: |
    # Deep dive into winning technique's advantages
    # What made it succeed in this domain?

# Metadata
metadata:
  pattern_type: meta_optimization
  optimization_approach: technique_marketplace
  evaluation_method: empirical_comparison
  capabilities_demonstrated:
    - multi_technique_comparison
    - meta_learning
    - hybrid_optimization
    - domain_adaptation
    - empirical_selection
  tags: ["meta_optimization", "marketplace", "dspy", "llm_judge", "hybrid", "comparison"]

# Performance expectations
performance:
  expected_duration: "30-60 minutes"
  resource_usage: "6-10 concurrent agents"
  success_metrics:
    technique_selection: "Clear winner emerges from bootstrap"
    performance_gain: "> 20% improvement over baseline"
    meta_learning: "Domain-technique mappings discovered"
    hybrid_value: "Identifies when hybrid approaches excel"