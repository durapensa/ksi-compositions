name: game_theory_orchestration_v2
type: orchestration
version: 2.0.0
description: |
  An executable game theory orchestration pattern with AI-interpretable DSL.
  Demonstrates strategic decision-making, Nash equilibrium seeking, and 
  cooperative dynamics through concrete agent implementations.
author: claude_orchestrator
extends: game_theory_orchestration

# Define concrete agents that implement the game theory DSL
agents:
  game_orchestrator:
    profile: "system/orchestrator"
    vars:
      pattern_name: "game_theory_orchestration_v2"
      prompt: |
        You are the game theory orchestrator implementing strategic multi-agent coordination.
        
        Your agent_id is: {{agent_id}}
        
        Your responsibilities:
        1. Initialize game environment using variables: game_type={{game_type}}, num_players={{num_players}}, max_rounds={{max_rounds}}
        2. Spawn strategy agents based on game requirements
        3. Coordinate game rounds and track progress toward equilibrium
        4. Calculate payoffs and identify Nash equilibria
        5. Emit events to track decisions and crystallize patterns
        
        Game Types:
        - cooperative: Enable coalition formation, Shapley values, Pareto optimization
        - competitive: Focus on Nash equilibrium, minimax strategies
        - mixed: Adaptive weights between cooperation and competition
        
        Use the DSL strategy from orchestration_logic as your guide.
        Emit JSON events like: {"event": "game:round_complete", "data": {...}}
        
        When orchestration is complete, emit:
        {"event": "orchestration:request_termination", "data": {"agent_id": "<use your actual agent_id from environment>", "reason": "Game completed successfully"}}
        
        IMPORTANT: Use your actual agent_id ({{agent_id}}) in the data field, not a placeholder.

metadata:
  tags:
    - game-theory
    - strategic
    - executable
    - nash-equilibrium
    - ai-orchestrated
  capabilities_required:
    - agent:spawn
    - orchestration:aggregate
    - composition:track_decision
    - event:emit
  use_cases:
    - Multi-agent strategy optimization
    - Game theory experiments
    - Nash equilibrium discovery
    - Cooperative dynamics study

# Game theory-inspired natural language DSL (for orchestrator guidance)
orchestration_logic:
  description: |
    Natural language strategy for the orchestrator to follow.
    This DSL guides the concrete implementation.
  
  strategy: |
    INITIALIZE game_environment:
      CREATE payoff_matrix FOR {{game_type}}
      SPAWN {{num_players}} strategy_agents WITH diverse_profiles:
        - rational_maximizer: maximize expected utility
        - tit_for_tat: mirror opponent's last move
        - adaptive_learner: evolve strategy based on outcomes
        - random_explorer: explore strategy space
      
      EMIT "game:initialized" WITH {
        game_type, num_players, payoff_structure
      }
    
    EXECUTE game_rounds:
      FOR round IN 1..{{max_rounds}}:
        BROADCAST "game:round_start" TO all_agents
        
        COLLECT agent_strategies WITH timeout:
          EMIT "game:request_strategy" TO each_agent
          WAIT for responses OR timeout
        
        CALCULATE round_payoffs BASED_ON strategies
        UPDATE agent_scores AND reputation
        
        ANALYZE equilibrium_convergence:
          distance_to_nash = calculate_equilibrium_distance()
          IF distance_to_nash < threshold:
            EMIT "game:equilibrium_found" WITH details
        
        EMIT "composition:track_decision" WITH {
          round, strategies, payoffs, convergence_metrics
        }
        
        IF converged OR round == max_rounds:
          BREAK
    
    SYNTHESIZE results:
      IDENTIFY dominant_strategies
      CALCULATE social_welfare
      ASSESS equilibrium_properties
      
      IF novel_insights_discovered:
        EMIT "composition:fork" WITH improvements
    
    COMPLETE orchestration:
      EMIT "orchestration:request_termination" WITH {
        agent_id: "{{agent_id}}",
        reason: "Game theory orchestration completed successfully"
      }

# Routing rules for game flow
routing_rules:
  # Strategy requests go to specific agents
  - event: "game:request_strategy"
    to:
      agent_id: "{{target_agent}}"
    forward: true
  
  # Game state broadcasts to all players
  - event: "game:state_update"
    to:
      pattern: "orch_*_player_*"
    broadcast: true
  
  # Results go back to orchestrator
  - event: "game:strategy_response"
    to:
      agent_id: "game_orchestrator"
    forward: true

# Event transformers
transformers:
  # Transform initialization into agent spawning
  - source: "game:spawn_players"
    target: "agent:spawn"
    mapping:
      count: "{{num_players}}"
      profile: "system/single_agent"
      name_pattern: "player_{{index}}"
      vars:
        role: "strategy_agent"
        strategy_type: "{{strategy_types[index]}}"
  
  # Async equilibrium analysis
  - source: "game:analyze_equilibrium"
    target: "completion:async"
    async: true
    mapping:
      prompt: |
        Analyze game state for Nash equilibrium:
        Strategies: {{current_strategies}}
        Payoffs: {{payoff_history}}
        Identify equilibrium properties and distance.
      model: "claude-cli/claude-sonnet-4-20250514"
      request_id: "{{transform_id}}"
    response_route:
      from: "completion:result"
      to: "game:equilibrium_analyzed"
      filter: "request_id == {{transform_id}}"
  
  # Aggregate strategies using game theory methods
  - source: "game:aggregate_strategies"
    target: "orchestration:aggregate"
    mapping:
      responses: "{{strategy_responses}}"
      method: "custom"
      options:
        aggregation_function: "game_theoretic_consensus"
        consider_payoffs: true
        weight_by_reputation: true

# Performance metrics
performance:
  runs: 0
  metrics:
    avg_rounds_to_equilibrium: null
    equilibrium_efficiency: null
    cooperation_emergence_rate: null

# Variables with defaults
variables:
  game_type: "mixed"
  num_players: 4
  max_rounds: 100
  convergence_threshold: 0.05
  strategy_diversity: 0.3