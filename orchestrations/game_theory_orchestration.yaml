name: game_theory_orchestration
type: orchestration
version: 1.0.0
description: |
  A game theory-inspired orchestration pattern demonstrating AI-interpretable DSL
  with strategic decision-making, Nash equilibrium seeking, and cooperative dynamics.
  Implements multi-agent coordination through game-theoretic principles.
author: claude_orchestrator
extends: null

# Define concrete agents that implement the game theory DSL
agents:
  game_orchestrator:
    profile: "base_orchestrator"
    vars:
      pattern_name: "game_theory_orchestration"
      initial_prompt: |
        You are the game theory orchestrator implementing the pattern's DSL strategy.
        
        Your role is to:
        1. Initialize the game environment based on the provided variables
        2. Spawn strategic agents (rational_maximizer, tit_for_tat, adaptive_learner, random_explorer)
        3. Orchestrate game rounds and track equilibrium convergence
        4. Analyze payoffs and identify Nash equilibria
        5. Track decisions and crystallize discovered patterns
        
        Variables available:
        - game_type: {{game_type}} (cooperative/competitive/mixed)
        - num_players: {{num_players}}
        - max_rounds: {{max_rounds}}
        
        Follow the game theory DSL strategy defined in orchestration_logic.
        Use event:emit to coordinate with other agents and track progress.
  
  # Template for strategic agents spawned by orchestrator
  strategy_agent_template:
    profile: "base_single_agent"
    template: true  # This is a template, not spawned directly
    vars:
      strategy_type: "{{strategy_type}}"
      initial_prompt: |
        You are a {{strategy_type}} strategic agent in a game theory scenario.
        Your objective is to maximize your payoff while following your strategy:
        
        - rational_maximizer: Always choose the action that maximizes expected utility
        - tit_for_tat: Cooperate first, then mirror opponent's last move
        - adaptive_learner: Learn from outcomes and adapt strategy over time
        - random_explorer: Mix strategies to explore the game space
        
        Respond to game state updates with your chosen action.

metadata:
  tags:
    - game-theory
    - strategic
    - cooperative
    - nash-equilibrium
    - ai-designed
  capabilities_required:
    - agent:spawn
    - orchestration:aggregate
    - composition:track_decision
    - event:emit
  use_cases:
    - Multi-agent strategy optimization
    - Resource allocation problems
    - Collaborative decision making
    - Nash equilibrium discovery

# Game theory-inspired natural language DSL
orchestration_logic:
  description: |
    This DSL implements game-theoretic concepts for multi-agent orchestration,
    focusing on strategic interactions, equilibrium discovery, and optimal
    coordination strategies.
  
  strategy: |
    INITIALIZE game_environment WITH context:
      DEFINE players = {{agents}} OR spawn_strategic_agents({{num_players}})
      ESTABLISH payoff_matrix FROM {{task_context}}
      SET game_type = {{game_type}} OR infer_from_context()
      
      IF game_type == "cooperative":
        CONFIGURE coalition_formation WITH shapley_values
        ENABLE pareto_optimization
      ELSIF game_type == "competitive":
        CONFIGURE nash_equilibrium_seeking
        ENABLE minimax_strategies
      ELSE:
        CONFIGURE mixed_strategy WITH adaptive_weights
    
    DEFINE strategic_decision_process:
      FOR each_decision_round:
        COLLECT agent_strategies USING "orchestration:gather"
        
        CALCULATE payoffs FOR all_strategy_combinations:
          EMIT "evaluation:compute_payoff" WITH {
            strategies: current_strategies,
            context: game_state,
            history: previous_rounds
          }
        
        IDENTIFY best_responses FOR each_agent:
          ANALYZE opponent_strategies
          COMPUTE expected_utilities
          ADJUST for_risk_preferences
        
        IF converging_to_equilibrium:
          TRACK equilibrium_metrics WITH {
            convergence_rate: calculate_convergence(),
            stability_measure: assess_stability(),
            social_welfare: sum(all_payoffs)
          }
        
        EMIT "composition:track_decision" WITH {
          round: current_round,
          strategies: agent_strategies,
          payoffs: calculated_payoffs,
          equilibrium_distance: distance_to_nash()
        }
    
    ORCHESTRATE strategic_interactions:
      SPAWN game_agents WITH strategic_profiles:
        - type: "rational_maximizer"
        - type: "tit_for_tat"
        - type: "adaptive_learner"
        - type: "random_explorer"
      
      IMPLEMENT iterated_game_rounds:
        WHILE not_converged AND rounds < {{max_rounds}}:
          BROADCAST game_state TO all_agents
          
          COLLECT agent_moves WITHIN timeout:
            EMIT "orchestration:request_move" WITH {
              game_state: current_state,
              history: move_history,
              payoff_structure: visible_payoffs
            }
          
          PROCESS moves WITH game_rules:
            VALIDATE legal_moves
            CALCULATE round_payoffs
            UPDATE agent_reputations
            
          IF cooperative_game:
            FACILITATE coalition_negotiations:
              ENABLE private_communication_channels
              TRACK coalition_proposals
              ENFORCE binding_agreements
          
          LEARN from_round_outcomes:
            UPDATE belief_models
            ADJUST strategy_weights
            DISCOVER emergent_patterns
    
    ANALYZE game_dynamics:
      COMPUTE equilibrium_properties:
        - efficiency: social_welfare / maximum_possible
        - fairness: gini_coefficient(payoff_distribution)
        - stability: perturbation_resistance
      
      IDENTIFY strategic_patterns:
        DETECT cycles IN strategy_evolution
        FIND dominant_strategies IF exist
        DISCOVER correlated_equilibria
      
      IF novel_equilibrium_discovered:
        CRYSTALLIZE game_pattern WITH {
          equilibrium_type: classification,
          discovery_path: strategy_evolution,
          applicability_conditions: context_requirements
        }
    
    AFTER game_completion:
      SYNTHESIZE strategic_insights:
        - optimal_strategy_profiles
        - equilibrium_characteristics
        - cooperation_emergence_conditions
      
      EVALUATE orchestration_effectiveness:
        - convergence_speed vs baseline
        - solution_quality vs theoretical_optimal
        - agent_satisfaction_metrics
      
      IF performance > threshold AND insights_valuable:
        EMIT "composition:fork" WITH {
          specialization: discovered_game_variant,
          improvements: strategic_optimizations,
          evidence: performance_metrics
        }

# Routing rules for game coordination
routing_rules:
  # Route strategy requests to appropriate agents
  - event: "game:request_strategy"
    to:
      agent_id: "{{player_id}}"
    forward: true
  
  # Broadcast game state updates to all strategy agents
  - event: "game:state_update"
    to:
      role: "strategy_agent"
    broadcast: true
  
  # Route analysis requests to orchestrator
  - event: "game:analyze"
    to:
      agent_id: "game_orchestrator"
    forward: true

# Event transformers for game-theoretic operations
transformers:
  # Transform strategy requests into agent-specific formats
  - source: "game:request_strategy"
    target: "orchestration:send"
    mapping:
      to:
        agent_id: "{{player_id}}"
      message:
        type: "strategy_request"
        game_state: "{{current_state}}"
        available_actions: "{{legal_moves}}"
        payoff_hint: "{{visible_payoffs}}"
  
  # Async payoff computation with AI assistance
  - source: "game:compute_complex_payoff"
    target: "completion:async"
    async: true
    mapping:
      prompt: |
        Compute payoffs for strategy profile:
        Players: {{players}}
        Strategies: {{strategies}}
        Context: {{game_context}}
        Consider: externalities, long-term effects, reputation
      model: "claude-cli/claude-sonnet-4-20250514"
      request_id: "{{transform_id}}"
      temperature: 0.2
    response_route:
      from: "completion:result"
      to: "game:payoff_computed"
      filter: "request_id == {{transform_id}}"
  
  # Aggregate strategies with game-specific methods
  - source: "game:aggregate_strategies"
    target: "orchestration:aggregate"
    mapping:
      responses: "{{strategy_proposals}}"
      method: "game_theoretic"
      options:
        aggregation_rule: "{{rule}}"  # nash, pareto, social_choice
        weight_by: "agent_reputation * historical_performance"
        handle_conflicts: "randomize_among_equilibria"
  
  # Coalition formation transformer
  - source: "game:form_coalition"
    target: "orchestration:create_channel"
    condition: "game_type == 'cooperative'"
    mapping:
      participants: "{{coalition_members}}"
      channel_type: "private_negotiation"
      rules:
        binding_agreements: true
        side_payments_allowed: "{{transferable_utility}}"
  
  # Equilibrium detection
  - source: "game:check_equilibrium"
    target: "orchestration:analyze"
    mapping:
      data: "{{strategy_history}}"
      analysis_type: "equilibrium_detection"
      criteria:
        - "nash_condition"
        - "pareto_optimality"
        - "coalition_stability"
  
  # Pattern crystallization for discovered equilibria
  - source: "game:crystallize_equilibrium"
    target: "composition:create"
    condition: "novel_equilibrium AND confidence > 0.85"
    mapping:
      name: "{{game_type}}_equilibrium_{{timestamp}}"
      type: "orchestration"
      category: "orchestrations"
      content:
        orchestration_logic:
          strategy: "{{discovered_strategy}}"
        metadata:
          game_type: "{{game_type}}"
          equilibrium_properties: "{{properties}}"
        lineage:
          discovered_from: "game_theory_orchestration"
          discovery_context: "{{game_context}}"

# Performance tracking for game outcomes
performance:
  runs: 0
  metrics:
    avg_convergence_time: null
    equilibrium_efficiency: null
    cooperation_rate: null
    strategic_diversity: null

# Learnings from game-theoretic orchestration
learnings:
  - insight: "Mixed strategies emerge naturally in competitive scenarios"
    confidence: 0.9
    evidence: "Design phase reasoning"
  - insight: "Reputation tracking improves cooperation rates"
    confidence: 0.85
    evidence: "Game theory literature"
  - insight: "Adaptive timeout strategies prevent exploitation"
    confidence: 0.8
    evidence: "Anticipated from iterated games"

# Runtime configuration variables
variables:
  default_game_type: "mixed"
  max_rounds: 100
  convergence_threshold: 0.95
  enable_coalition_formation: true
  reputation_decay_rate: 0.1
  exploration_rate: 0.15
  payoff_visibility: "partial"