name: adaptive_pipeline
type: orchestration
version: 1.0.0
description: "Adaptive pipeline orchestration that adjusts stages based on intermediate results"
author: pipeline_orchestrator_3a2f

# Agent topology for pipeline stages
agents:
  pipeline_controller:
    profile: base_orchestrator
    prompt_template: pipeline_control
    vars:
      optimization_mode: throughput
      
  stage_agents:
    template: "stage_{n}"
    count: "dynamic"  # Determined by orchestrator
    profile: base_single_agent

# DSL for pipeline orchestration
orchestration_logic:
  description: |
    Adaptive pipeline that can add, remove, or modify stages based on data characteristics
    and intermediate results. The orchestrator interprets this strategy dynamically.
  
  strategy: |
    # Adaptive Pipeline Strategy
    
    INITIALIZE pipeline WITH input_data:
      ANALYZE data_characteristics
      DETERMINE initial_stages BASED ON:
        - data.complexity
        - data.volume  
        - quality_requirements
        - time_constraints
    
    STAGE preprocessing:
      IF data.format IN ['messy', 'unstructured']:
        ADD cleaning_stage WITH aggressive_cleaning
      IF data.size > threshold:
        ENABLE parallel_processing WITH batch_size = optimal_chunks(data)
      
      SPAWN stage_1_agent WITH profile: data_cleaner
      PROCESS data THROUGH stage_1_agent
      
      CHECKPOINT results AS cleaned_data
      MEASURE quality_metrics
      
    STAGE analysis:
      BRANCH based_on preprocessing.quality_metrics:
        CASE high_quality:
          PROCEED to advanced_analysis
        CASE medium_quality:
          ADD validation_stage BEFORE analysis
        CASE low_quality:
          RETRY preprocessing WITH stricter_parameters
          OR FAIL_GRACEFULLY WITH explanation
      
      DYNAMICALLY SELECT analysis_agents BASED ON:
        - data.patterns_detected
        - required_outputs
        - available_compute
      
      SPAWN selected_agents IN parallel_where_possible
      
    STAGE synthesis:
      WHEN all_analyses_complete:
        DETERMINE synthesis_strategy FROM results.variance
        
        IF results.consensus > 0.8:
          USE simple_aggregation
        ELIF results.complementary:
          USE weighted_combination
        ELSE:
          SPAWN expert_synthesizer FOR complex_merge
      
    ADAPT pipeline IN_REAL_TIME:
      MONITOR stage_performance EVERY completion
      
      IF stage.bottleneck_detected:
        EITHER scale_horizontally WITH more_agents
        OR optimize_stage WITH better_algorithm
        
      IF stage.consistently_fast:
        CONSIDER merging WITH next_stage
        
      IF quality_degradation:
        INSERT quality_check_stage AFTER problematic_stage
    
    COMPLETE pipeline:
      GENERATE summary_report
      TRACK all_decisions WITH outcomes
      IF performance > baseline:
        DOCUMENT optimizations AS learnings
        CONSIDER pattern_fork FOR specialization
  
  stage_templates:
    - name: "data_validation"
      condition: "data.quality < 0.7"
      purpose: "Ensure data meets minimum quality standards"
      
    - name: "parallel_processor"
      condition: "data.size > 10000 AND time_limit < 5min"
      purpose: "Split processing across multiple agents"
      
    - name: "quality_gate"
      condition: "critical_application = true"
      purpose: "Enforce strict quality thresholds"

# Routing with stage-aware rules
routing:
  rules:
    - pattern: "stage:output"
      from: "stage_*"
      to: "pipeline_controller"
      then: "next_stage OR complete"
      
    - pattern: "stage:error"
      from: "stage_*"
      to: "pipeline_controller"
      action: "error_recovery"
      
    - pattern: "pipeline:adapt"
      from: "pipeline_controller"
      broadcast: true

# Coordination for pipeline flow
coordination:
  flow_control:
    mode: "sequential_with_branches"
    parallelism: "auto_detect"
    checkpoint_frequency: "after_each_stage"
    
  error_handling:
    stage_failure: "retry_or_skip"
    data_corruption: "rollback_to_checkpoint"
    timeout: "graceful_degradation"
    
  optimization:
    monitor_metrics:
      - throughput
      - quality_score
      - resource_usage
    adapt_when:
      - "bottleneck_detected"
      - "quality_threshold_missed"
      - "resource_limit_approached"

# Performance metrics
performance:
  runs: 47
  avg_throughput: "1250 items/min"
  quality_score: 0.89
  adaptations_per_run: 2.3
  
  stage_metrics:
    preprocessing:
      avg_duration: "12s"
      success_rate: 0.96
    analysis:
      avg_duration: "45s"
      parallelism_gain: "3.2x"
    synthesis:
      avg_duration: "8s"
      quality_improvement: "+15%"

# Learnings from pipeline operations
learnings:
  - insight: "Parallel preprocessing only helps with >1000 items"
    confidence: 0.91
    evidence: "A/B testing across 23 runs"
    impact: "Skip parallelization overhead for small batches"
    
  - insight: "Quality gates after each stage reduce overall errors by 40%"
    confidence: 0.87
    evidence: "Comparative analysis"
    recommendation: "Default to quality gates for critical pipelines"
    
  - insight: "Dynamic stage addition based on data profile improves quality"
    confidence: 0.79
    evidence: "Pattern analysis of 47 runs"
    discovered_by: "pipeline_orchestrator_3a2f"

# Resource management
resources:
  limits:
    max_concurrent_stages: 5
    max_agents_per_stage: 10
    memory_per_agent: "512MB"
    
  scaling_rules:
    - trigger: "queue_depth > 100"
      action: "scale_stage horizontal"
      
    - trigger: "memory_usage > 80%"
      action: "reduce_batch_size"

# Metadata for discovery
metadata:
  tags: ["pipeline", "adaptive", "data-processing", "quality-aware"]
  capabilities_required: ["agent:spawn", "state:checkpoint", "monitoring:metrics"]
  
  use_cases:
    - "Data processing pipelines"
    - "Multi-stage analysis workflows"
    - "Quality-assured transformations"
    - "Adaptive ETL processes"
    
  selection_criteria:
    task_keywords: ["pipeline", "stages", "sequential", "transform", "process"]
    data_volume: "medium-high"
    quality_requirements: "configurable"
    
  compatibility:
    works_well_with: ["data_validator", "quality_assessor", "performance_monitor"]
    not_recommended_for: ["simple_tasks", "single_operation"]